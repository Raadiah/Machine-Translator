{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\n\ntry:\n  # %tensorflow_version only exists in Colab.\n  %tensorflow_version 2.x\nexcept Exception:\n  pass\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\n\nimport unicodedata\nimport re\nimport io\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download the file\n\nfrom os import path\n\n#path = /gdrive/My Drive/dataset\n\n#path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\" #/gdrive/My Drive/dataset\n\npath_to_en_file = '/kaggle/input/SUPara0.8M_en.txt'\n\npath_to_bn_file = '/kaggle/input/SUPara0.8M_bn.txt'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converts the unicode file to ascii\ndef unicode_to_ascii(s):\n  return ''.join(c for c in unicodedata.normalize('NFD', s)\n      if unicodedata.category(c) != 'Mn')\n\n\ndef preprocess_sentence(w, en):\n  w = unicode_to_ascii(w.lower().strip())\n\n  # creating a space between a word and the punctuation following it\n  # eg: \"he is a boy.\" => \"he is a boy .\"\n  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n  w = re.sub(r\"([?.!,।])\", r\" \\1 \", w)\n  w = re.sub(r'[\" \"]+', \" \", w)\n\n  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\" \"।\")\n  if en:\n    w = re.sub(r\"[^a-zA-Z?.!,0-9]+\", \" \", w)\n\n\n  w = w.rstrip().strip()\n\n  # adding a start and an end token to the sentence\n  # so that the model know when to start and stop predicting.\n  w = '<start> ' + w + ' <end>'\n  return w","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Remove the accents\n# 2. Clean the sentences\n# 3. Return word pairs in the format: [ENGLISH, SPANISH]\ndef create_dataset(path, en, num_examples):\n  lines_source = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n\n  word_pair = ['<start><end>']\n\n  for l in lines_source:\n    w = preprocess_sentence(l, en)\n    word_pair.append(w)\n  \n\n  return word_pair","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"en = create_dataset(path_to_en_file, 1, None)\nbn = create_dataset(path_to_bn_file, 0, None)\n\nprint(en[6])\nprint(bn[6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def max_length(tensor):\n  return max(len(t) for t in tensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(lang, total_lang):\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n      filters='')\n  \n  #updates internal vocabulary according to list lang\n  lang_tokenizer.fit_on_texts(total_lang)\n\n  #convers lang to a sequence of integers\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n\n  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n                                                         padding='post')\n\n  #lang_tokenizer is the vocabulary, tensor is integer sequence\n  return tensor, lang_tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset(path_src, path_target, num_examples=None):\n  # creating cleaned input, output pairs\n    src_lang = create_dataset(path_src, 1, num_examples)\n    targ_lang = create_dataset(path_target, 0, num_examples)\n    \n    input_train, input_test, target_train, target_test = train_test_split(src_lang, targ_lang, test_size=0.0071)\n    \n    input_tensor, inp_lang_tokenizer = tokenize(input_train, src_lang)\n    target_tensor, targ_lang_tokenizer = tokenize(target_train, targ_lang)\n    \n    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer, input_test, target_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try experimenting with the size of that dataset\nnum_examples = 70816\ninput_tensor_train, target_tensor_train, inp_lang, targ_lang, input_test, target_test = load_dataset(path_to_en_file, path_to_bn_file, num_examples)\n\n# Calculate max_length of the target tensors\nmax_length_targ, max_length_inp = max_length(target_tensor_train), max_length(input_tensor_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating training and validation sets using an 80-20 split\n#input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n\n# Show length\nprint(len(input_tensor_train), len(input_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(input_test[8])\nprint(target_test[8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert(lang, tensor):\n  for t in tensor:\n    if t!=0:\n      print (\"%d ----> %s\" % (t, lang.index_word[t]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Input Language; index to word mapping\")\nconvert(inp_lang, input_tensor_train[0])\nprint ()\nprint (\"Target Language; index to word mapping\")\nconvert(targ_lang, target_tensor_train[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 32\nsteps_per_epoch = len(input_tensor_train)//BATCH_SIZE\nembedding_dim = 300\nunits = 300\nvocab_inp_size = len(inp_lang.word_index)+1\nvocab_tar_size = len(targ_lang.word_index)+1\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_input_batch, example_target_batch = next(iter(dataset))\nexample_input_batch.shape, example_target_batch.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Encoder Decoder Attention**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   dropout=0.2,\n                                   recurrent_dropout=0.5,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n  def call(self, x, hidden):\n    x = self.embedding(x)\n    output, state = self.gru(x, initial_state = hidden)\n    return output, state\n\n  def initialize_hidden_state(self):\n    return tf.zeros((self.batch_sz, self.enc_units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n\n# sample input\nsample_hidden = encoder.initialize_hidden_state()\nsample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\nprint ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\nprint ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, query, values):\n    # hidden shape == (batch_size, hidden size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n    # we are doing this to perform addition to calculate the score\n    hidden_with_time_axis = tf.expand_dims(query, 1)\n\n    # score shape == (batch_size, max_length, 1)\n    # we get 1 at the last axis because we are applying score to self.V\n    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n    score = self.V(tf.nn.tanh(\n        self.W1(values) + self.W2(hidden_with_time_axis)))\n\n    # attention_weights shape == (batch_size, max_length, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * values\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attention_layer = BahdanauAttention(128)\nattention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n\nprint(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\nprint(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   dropout=0.2,\n                                   recurrent_dropout=0.5,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n\n    # used for attention\n    self.attention = BahdanauAttention(self.dec_units)\n\n  def call(self, x, hidden, enc_output):\n    # enc_output shape == (batch_size, max_length, hidden_size)\n    context_vector, attention_weights = self.attention(hidden, enc_output)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # output shape == (batch_size * 1, hidden_size)\n    output = tf.reshape(output, (-1, output.shape[2]))\n\n    # output shape == (batch_size, vocab)\n    x = self.fc(output)\n\n    return x, state, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\nsample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n                                      sample_hidden, sample_output)\n\nprint ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optimizer and Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n  loss = 0\n\n  with tf.GradientTape() as tape:\n    enc_output, enc_hidden = encoder(inp, enc_hidden)\n\n    dec_hidden = enc_hidden\n\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n\n    # Teacher forcing - feeding the target as the next input\n    for t in range(1, targ.shape[1]):\n      # passing enc_output to the decoder\n      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n\n      loss += loss_function(targ[:, t], predictions)\n\n      # using teacher forcing\n      dec_input = tf.expand_dims(targ[:, t], 1)\n\n  batch_loss = (loss / int(targ.shape[1]))\n\n  variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, variables)\n\n  optimizer.apply_gradients(zip(gradients, variables))\n\n  return batch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\ntf.keras.backend.set_learning_phase(1)\n\nfor epoch in range(EPOCHS):\n  start = time.time()\n\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    total_loss += batch_loss\n\n    if batch % 100 == 0:\n      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                   batch,\n                                                   batch_loss.numpy()))\n  # saving (checkpoint) the model every 2 epochs\n  if (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix = checkpoint_prefix)\n\n  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss / steps_per_epoch))\n  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Translate"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nfrom nltk import ngrams\n\nchencherry = SmoothingFunction()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length_targ = 40\n\ndef evaluate(sentence, reference):\n  tf.keras.backend.set_learning_phase(0)\n  #attention_plot = np.zeros((max_length_targ, max_length_inp))\n\n  #sentence = preprocess_sentence(sentence, 1)\n  \n  inp_len = 0  \n  inputs = []\n  for i in sentence.split(' '):    \n    try:\n        x = inp_lang.word_index[i]\n        inputs.append(x)\n    except KeyError:\n        pass\n\n        \n        \n  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                         maxlen=max_length_inp,\n                                                         padding='post')\n  inputs = tf.convert_to_tensor(inputs)\n\n  result = \"\"\n\n  hidden = [tf.zeros((1, units))]\n  enc_out, enc_hidden = encoder(inputs, hidden)\n\n  dec_hidden = enc_hidden\n  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n\n  for t in range(max_length_targ):\n    predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                         dec_hidden,\n                                                         enc_out)\n\n    # storing the attention weights to plot later on\n    attention_weights = tf.reshape(attention_weights, (-1, ))\n    #attention_plot[t] = attention_weights.numpy()\n\n    predicted_id = tf.argmax(predictions[0]).numpy()\n\n\n    if targ_lang.index_word[predicted_id] == '<end>':\n      score = sentence_bleu(reference.split(), result.split(), smoothing_function=chencherry.method1, auto_reweigh= True)\n      print(inp_len)\n      return result, sentence, score\n\n    else:\n        result += targ_lang.index_word[predicted_id] + \" \"\n        inp_len = inp_len + 1\n\n    # the predicted ID is fed back into the model\n    dec_input = tf.expand_dims([predicted_id], 0)\n    score = sentence_bleu(reference.split(), result.split(), smoothing_function=chencherry.method1, auto_reweigh= True)\n\n  return result, sentence, score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for plotting the attention weights\n#def plot_attention(attention, sentence, predicted_sentence):\n # fig = plt.figure(figsize=(10,10))\n  #ax = fig.add_subplot(1, 1, 1)\n  #ax.matshow(attention, cmap='viridis')\n\n  #fontdict = {'fontsize': 14}\n\n  #ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n  #ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n\n  #ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n  #ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n  #plt.show()\n  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def translate(sentence, reference):\n  result, sentence, score = evaluate(sentence, reference)\n\n  print('Input: %s' % (sentence))\n  print('Predicted translation: {}'.format(result))\n  print(score)\n  return score\n  #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n  #plot_attention(attention_plot, sentence.split(' '), result.split(' '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"translate(\"i eat rice\", \"আমি ভাত খাই ।\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BLEU Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = sentence_bleu(\"আমি ভাত খাই ।\".split(), \"আমি ভাত খাই \".split(), auto_reweigh= True)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loop = 0\nbleu = 0\n\nfor t in input_test:\n    candidate = target_test[loop]\n    reference = input_test[loop]\n    candidate = re.sub(\"<start> \", \"\", candidate)\n    candidate = re.sub(\" <end>\", \"\", candidate)\n    print(candidate)\n    score = translate(reference, candidate)\n    loop = loop + 1\n    bleu = bleu + score\n\n\n    \n#score = sentence_bleu(reference, candidate, smoothing_function=chencherry.method1)\n#print(score)\n\n#print(list(ngrams(reference, 2)))\n#print(list(ngrams(reference, 3)))\n    \nbleu = bleu/loop\nprint(bleu)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}